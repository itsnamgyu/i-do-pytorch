{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "artistic-standard",
   "metadata": {},
   "source": [
    "## Torch Autograd\n",
    "\n",
    "Supports automatic computation of gradient \"for any computation graph\".\n",
    "I assume all operations are differentiable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "checked-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "honest-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5) + 3\n",
    "y = torch.zeros(3)\n",
    "w = torch.randn(5, 3, requires_grad=True)  # note, default is false\n",
    "b = torch.randn(3)\n",
    "b.requires_grad_(True)  # you can set grad requirements afterwards\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "formal-response",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9705, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-survey",
   "metadata": {},
   "source": [
    "### Computation Graph\n",
    "![](https://pytorch.org/tutorials/_images/comp-graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "freelance-system",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<AddBackward0 at 0x7fd4636dfd90>,\n",
       " <BinaryCrossEntropyWithLogitsBackward at 0x7fd4636df220>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad_fn, loss.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "contained-lotus",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()  # compute gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "framed-fourth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0422, 1.3333, 1.3333],\n",
       "         [0.0422, 1.3333, 1.3333],\n",
       "         [0.0422, 1.3333, 1.3333],\n",
       "         [0.0422, 1.3333, 1.3333],\n",
       "         [0.0422, 1.3333, 1.3333]]),\n",
       " tensor([0.0106, 0.3333, 0.3333]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad, b.grad  # grads are equal because x is all ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-dictionary",
   "metadata": {},
   "source": [
    "### Conjuncture on Implementation\n",
    "\n",
    "To obtain gradients for child nodes, you simply need to multiply the derivate w.r.t. the operation.\n",
    "A full backprop would require computing all intermediate function values and the gradients\n",
    "for the leaf nodes (inputs and weights) that _`requires_grad`_ (oh!).\n",
    "\n",
    "I.e., `n.backward()` computes dn/di for all nodes i that `requires_grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "civilian-atlas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: grad can be implicitly created only for scalar outputs\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    z.backward()\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-investor",
   "metadata": {},
   "source": [
    "### `backward()` Function\n",
    "\n",
    "Does backward propagation starting from current scalar tensor assuming that the value is the loss.\n",
    "\n",
    "- Q. What even is the definition of backprop?\n",
    "- A. Finding the gradients of some input variable w.r.t. the cost function.\n",
    "  Thus, recall, the cost value does not affect the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-nature",
   "metadata": {},
   "source": [
    "### no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "sustainable-collective",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b  # requires grad because non-zero leaf nodes requires_grad?\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "treated-concentrate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "    print(z.requires_grad)\n",
    "    print(w.requires_grad)\n",
    "    print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-incident",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "`no_grad()` disables `requires_grad` for any new nodes (of a bigger computation graph).\n",
    "Of course, leaf nodes preserve their `requires_grad`.\n",
    "\n",
    "- Q. what about intermediate notes?\n",
    "- A. yes! Any previously defined(?) nodes preserve their `requires_grad` (as they should)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "signed-permission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "with torch.no_grad():\n",
    "    zz = torch.matmul(z, z)\n",
    "z.requires_grad, zz.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-connectivity",
   "metadata": {},
   "source": [
    "### `detach()`\n",
    "\n",
    "Acquires a reference to the same tensor with `requires_grad` turned off.\n",
    "Note that memory is shared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "heavy-narrative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zd = z.detach()\n",
    "zd.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "asian-chancellor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-18.3748,  -3.6277,   5.8826], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "aggregate-penalty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-18.3748,  -3.6277,   5.8826])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "colored-diversity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-16.3748,  -1.6277,   7.8826])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zd.add_(1)\n",
    "zd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "explicit-dublin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-16.3748,  -1.6277,   7.8826], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-survivor",
   "metadata": {},
   "source": [
    "### Intermediate Break in `requires_grad`?\n",
    "\n",
    "Will disable backprop to child nodes (because how would that happen?).\n",
    "I'm assuming that there is some traversal logic that breaks the loop\n",
    "when encountering a node that doesn't `requires_grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "needed-convention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False True\n"
     ]
    }
   ],
   "source": [
    "one = torch.ones(3)\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "zz = torch.matmul(z, one)\n",
    "zz.requires_grad_(True)\n",
    "print(z.requires_grad, zz.requires_grad)\n",
    "zz.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "quantitative-fiber",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, tensor(1.), None)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad, z.grad, zz.grad, w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca13172",
   "metadata": {},
   "source": [
    "Yes, there must be no breaks in `requires_grad` for full backprop (obviously)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
